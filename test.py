import torch
import sys
from unsloth import FastLanguageModel
from colorama import init, Fore, Style
from peft import PeftModel

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ü–≤–µ—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
init(autoreset=True)

def load_model():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å LoRA-–∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏"""
    print(Fore.CYAN + "üöÄ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å...")
    
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
        base_model, tokenizer = FastLanguageModel.from_pretrained(
            model_name="unsloth/Meta-Llama-3.1-8B-bnb-4bit",
            max_seq_length=1024,
            load_in_4bit=True,
            dtype=None,
        )
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ –∞–¥–∞–ø—Ç–µ—Ä—ã
        print("–ó–∞–≥—Ä—É–∂–∞—é LoRA-–∞–¥–∞–ø—Ç–µ—Ä—ã —á–µ—Ä–µ–∑ PeftModel...")
        model = PeftModel.from_pretrained(
            base_model,
            "./finetuned_llama3_cosmetology_model",  # ‚Üê –í–∞—à–∞ –ø–∞–ø–∫–∞ —Å –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏
            adapter_name="cosmetology_lora"
        )
        
        # 3. –ü–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        model.eval()
        
        print(Fore.GREEN + "‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        return model, tokenizer
        
    except Exception as e:
        print(Fore.RED + f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        print(Fore.YELLOW + "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å: ./finetuned_llama3_cosmetology_model")
        return None, None

def chat_loop(model, tokenizer):
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —á–∞—Ç–∞"""
    print(Fore.CYAN + "\n" + "="*60)
    print(Fore.CYAN + "üí¨ –ß–ê–¢ –° –ú–û–î–ï–õ–¨–Æ (Ctrl+C –¥–ª—è –≤—ã—Ö–æ–¥–∞)")
    print(Fore.CYAN + "="*60)
    
    # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    system_prompt = (
        "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –∫–æ—Å–º–µ—Ç–æ–ª–æ–≥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –£ —Ç–µ–±—è –µ—Å—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: "
        "\"calculator\" (–¥–ª—è —Ä–∞—Å—á—ë—Ç–æ–≤) –∏ \"call_DB\" (–¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö). "
        "–û—Ç–≤–µ—á–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n\n"
    )
    
    # –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ (–º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)
    conversation = []
    
    while True:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤–æ–ø—Ä–æ—Å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            print(Fore.YELLOW + "\n[–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ '–≤—ã—Ö–æ–¥'/quit]:")
            user_input = input(Fore.WHITE + "üë§ –í—ã: ").strip()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—ã—Ö–æ–¥
            if user_input.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', 'q']:
                print(Fore.CYAN + "\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            if not user_input:
                continue
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            conversation.append(f"user: {user_input}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            full_prompt = system_prompt + "\n".join(conversation[-4:]) + "\nassistant: "
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = tokenizer(
                [full_prompt],
                return_tensors="pt",
                truncation=True,
                max_length=1024
            ).to("cuda")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º
            print(Fore.BLUE + "\nü§ñ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç...", end="", flush=True)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=256,           # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
                    temperature=0.7,              # –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                    do_sample=True,               # –°—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
                    top_p=0.9,                    # –ö–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞
                    repetition_penalty=1.1,       # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä—ã
                    pad_token_id=tokenizer.eos_token_id,
                )
            
            print(Fore.GREEN + " ‚úÖ")
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–π)
            assistant_response = full_response.split("assistant:")[-1].strip()
            
            # –í—ã–≤–æ–¥–∏–º –æ—Ç–≤–µ—Ç –∫—Ä–∞—Å–∏–≤–æ
            print(Fore.GREEN + f"ü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {assistant_response}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
            conversation.append(f"assistant: {assistant_response}")
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –ø–∞—Ä—ã)
            if len(conversation) > 6:
                conversation = conversation[-6:]
                
        except KeyboardInterrupt:
            print(Fore.CYAN + "\n\nüëã –î–∏–∞–ª–æ–≥ –∑–∞–≤–µ—Ä—à–µ–Ω –ø–æ –∑–∞–ø—Ä–æ—Å—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.")
            break
        except Exception as e:
            print(Fore.RED + f"\n‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
            continue

def quick_test(model, tokenizer):
    """–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ–¥ —á–∞—Ç–æ–º"""
    print(Fore.CYAN + "\nüß™ –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏...")
    
    test_prompts = [
        "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–∏—Å–ª–æ—Ç–Ω—ã–π –ø–∏–ª–∏–Ω–≥?",
        "–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 15 + 27?",
    ]
    
    for prompt in test_prompts:
        full_prompt = f"user: {prompt}\nassistant: "
        inputs = tokenizer([full_prompt], return_tensors="pt").to("cuda")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response.split("assistant:")[-1].strip()
        
        print(Fore.WHITE + f"\nQ: {prompt}")
        print(Fore.GREEN + f"A: {answer[:100]}..." if len(answer) > 100 else f"A: {answer}")
    
    print(Fore.GREEN + "\n‚úÖ –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print(Fore.CYAN + "="*60)
    print(Fore.CYAN + "ü§ñ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –ß–ê–¢ –° –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–¨–Æ")
    print(Fore.CYAN + "="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model, tokenizer = load_model()
    if model is None:
        return
    
    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    quick_test(model, tokenizer)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —á–∞—Ç
    chat_loop(model, tokenizer)
    
    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ
    torch.cuda.empty_cache()
    print(Fore.CYAN + "\nüíæ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞.")

if __name__ == "__main__":
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É UTF-8 –¥–ª—è Windows
    if sys.platform == "win32":
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    
    main()